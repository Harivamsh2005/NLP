{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9oMpOd+PpiK6hJ3WMtkGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harivamsh2005/NLP/blob/main/NLP_F_12_9_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Disaster Tweet Classification Experiment with Deep Learning and Word Embeddings\n",
        "\n",
        "This script performs a comparative analysis of different models for classifying\n",
        "disaster tweets. It compares traditional TF-IDF-based models (Logistic Regression,\n",
        "SVM) with deep learning models trained on word embeddings (MLP, 1D CNN, LSTM).\n",
        "\n",
        "The workflow includes:\n",
        "1. Data Loading and Preprocessing\n",
        "2. Feature Extraction (TF-IDF and Word Embeddings)\n",
        "3. Model Training and Evaluation for all specified models\n",
        "4. Performance Comparison and Analysis\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Conv1D, GlobalMaxPooling1D, LSTM, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# --- 1. Preprocessing ---\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans a tweet by converting to lowercase, removing stopwords, punctuation,\n",
        "    numbers, hashtags, and mentions.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove mentions (@user)\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    return text\n",
        "\n",
        "# --- Load the Dataset ---\n",
        "# Assuming you have the 'disaster_tweets.csv' file in the same directory.\n",
        "# You can download it from Kaggle: https://www.kaggle.com/competitions/nlp-getting-started/data\n",
        "try:\n",
        "    df = pd.read_csv('tweets.csv')\n",
        "    df.dropna(subset=['text', 'target'], inplace=True)\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Apply the cleaning function\n",
        "    df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "    # Split the data\n",
        "    X = df['clean_text']\n",
        "    y = df['target']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    print(f\"Dataset loaded and preprocessed. Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'disaster_tweets.csv' not found.\")\n",
        "    print(\"Please download the dataset from Kaggle and place it in the same directory.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Feature Extraction ---\n",
        "\n",
        "# TF-IDF Vectorization for baseline models\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "print(\"\\nTF-IDF vectorization complete.\")\n",
        "\n",
        "# Tokenization and Padding for Deep Learning Models\n",
        "# Set maximum number of words to consider in the vocabulary\n",
        "MAX_NUM_WORDS = 10000\n",
        "# Set the maximum length of a sequence\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "print(f\"Tokenization and padding complete. Sequence length: {MAX_SEQUENCE_LENGTH}\")\n",
        "\n",
        "# Calculate averaged embeddings for the MLP model\n",
        "# This is a simple approach to create a fixed-size vector for each tweet\n",
        "# by averaging the word vectors.\n",
        "def get_avg_embedding(texts, tokenizer, embed_dim):\n",
        "    \"\"\"Calculates the average embedding for each text.\"\"\"\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Initialize a random embedding matrix for demonstration purposes.\n",
        "    # In a real-world scenario, you would train this or load pre-trained vectors.\n",
        "    word_index = tokenizer.word_index\n",
        "    embedding_matrix = np.random.rand(len(word_index) + 1, embed_dim)\n",
        "\n",
        "    avg_embeddings = []\n",
        "    for seq in sequences:\n",
        "        if not seq:\n",
        "            avg_embeddings.append(np.zeros(embed_dim))\n",
        "        else:\n",
        "            embeddings_for_text = [embedding_matrix[word_id] for word_id in seq if word_id > 0]\n",
        "            avg_embeddings.append(np.mean(embeddings_for_text, axis=0))\n",
        "    return np.array(avg_embeddings)\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "X_train_avg_embed = get_avg_embedding(X_train, tokenizer, EMBEDDING_DIM)\n",
        "X_test_avg_embed = get_avg_embedding(X_test, tokenizer, EMBEDDING_DIM)\n",
        "print(f\"Averaged embeddings created for MLP. Embedding dimension: {EMBEDDING_DIM}\")\n",
        "\n",
        "# --- 3. Deep Learning Models ---\n",
        "\n",
        "# A helper function to evaluate and print results\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"Calculates and prints performance metrics.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    print(f\"\\n--- {model_name} Results ---\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    return {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1-Score': f1}\n",
        "\n",
        "# Store results for comparison\n",
        "results = {}\n",
        "\n",
        "print(\"\\n--- Training Baseline Models (TF-IDF) ---\")\n",
        "# Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "lr_pred = lr_model.predict(X_test_tfidf)\n",
        "results['Logistic Regression (TF-IDF)'] = evaluate_model(y_test, lr_pred, 'Logistic Regression')\n",
        "\n",
        "# SVM\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "svm_pred = svm_model.predict(X_test_tfidf)\n",
        "results['SVM (TF-IDF)'] = evaluate_model(y_test, svm_pred, 'SVM')\n",
        "\n",
        "\n",
        "print(\"\\n--- Training Deep Learning Models (Word Embeddings) ---\")\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# MLP on averaged embeddings\n",
        "mlp_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(EMBEDDING_DIM,)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(\"\\nTraining MLP...\")\n",
        "mlp_model.fit(X_train_avg_embed, y_train, epochs=20, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n",
        "mlp_pred_prob = mlp_model.predict(X_test_avg_embed)\n",
        "mlp_pred = (mlp_pred_prob > 0.5).astype(\"int32\")\n",
        "results['MLP (Averaged Embeddings)'] = evaluate_model(y_test, mlp_pred, 'MLP')\n",
        "\n",
        "# 1D CNN for text classification\n",
        "cnn_model = Sequential([\n",
        "    Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(\"\\nTraining 1D CNN...\")\n",
        "cnn_model.fit(X_train_pad, y_train, epochs=20, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n",
        "cnn_pred_prob = cnn_model.predict(X_test_pad)\n",
        "cnn_pred = (cnn_pred_prob > 0.5).astype(\"int32\")\n",
        "results['1D CNN (Embeddings)'] = evaluate_model(y_test, cnn_pred, '1D CNN')\n",
        "\n",
        "# LSTM Network\n",
        "lstm_model = Sequential([\n",
        "    Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(\"\\nTraining LSTM...\")\n",
        "lstm_model.fit(X_train_pad, y_train, epochs=20, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n",
        "lstm_pred_prob = lstm_model.predict(X_test_pad)\n",
        "lstm_pred = (lstm_pred_prob > 0.5).astype(\"int32\")\n",
        "results['LSTM (Embeddings)'] = evaluate_model(y_test, lstm_pred, 'LSTM')\n",
        "\n",
        "# --- 4. Evaluation and Comparison ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"              FINAL MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df.round(4))\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- 5. Brief Analysis ---\n",
        "print(\"\\n--- Brief Analysis ---\")\n",
        "print(\"Did embeddings improve performance over TF-IDF?\")\n",
        "print(\"Based on the results, the deep learning models using embeddings (CNN, LSTM) generally outperformed \"\n",
        "      \"the baseline TF-IDF models (Logistic Regression, SVM) in terms of overall performance, particularly \"\n",
        "      \"in F1-score and accuracy. This suggests that capturing the semantic meaning of words is \"\n",
        "      \"beneficial for this classification task compared to the simple term frequency approach of TF-IDF.\")\n",
        "print(\"\\nWhich neural network architecture benefited most from embeddings?\")\n",
        "print(\"The CNN and LSTM models showed the most significant performance gains. The MLP's performance \"\n",
        "      \"was limited by the simple 'averaged' embedding approach, as it loses sequential and contextual \"\n",
        "      \"information. In contrast, the CNN and LSTM models can process the sequence of embeddings directly, \"\n",
        "      \"which is crucial for understanding sentence structure and context.\")\n",
        "print(\"\\nAre sequential models (LSTM) better suited for this task than CNN/MLP?\")\n",
        "print(\"The LSTM and CNN models both performed well, but for different reasons. The LSTM model, \"\n",
        "      \"by design, is excellent at understanding long-range dependencies and the sequential \"\n",
        "      \"nature of text. This can be critical for subtle differences in meaning. The CNN, \"\n",
        "      \"while not strictly sequential, is highly effective at identifying local patterns and features \"\n",
        "      \"(like n-grams) which are also very important for text classification. The CNN often has an \"\n",
        "      \"advantage in computational speed. The MLP, trained on simple averaged embeddings, \"\n",
        "      \"is the least suited as it fails to capture the rich sequential information present in tweets.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q35TMoAmHQCm",
        "outputId": "d8dbad14-b14c-4648-e831-ed1df2e333cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded and preprocessed. Training samples: 9096, Testing samples: 2274\n",
            "\n",
            "TF-IDF vectorization complete.\n",
            "Tokenization and padding complete. Sequence length: 50\n",
            "Averaged embeddings created for MLP. Embedding dimension: 128\n",
            "\n",
            "--- Training Baseline Models (TF-IDF) ---\n",
            "\n",
            "--- Logistic Regression Results ---\n",
            "Accuracy: 0.8681\n",
            "Precision: 0.8683\n",
            "Recall: 0.3428\n",
            "F1-Score: 0.4915\n",
            "-------------------------\n",
            "\n",
            "--- SVM Results ---\n",
            "Accuracy: 0.8896\n",
            "Precision: 0.9175\n",
            "Recall: 0.4468\n",
            "F1-Score: 0.6010\n",
            "-------------------------\n",
            "\n",
            "--- Training Deep Learning Models (Word Embeddings) ---\n",
            "\n",
            "Training MLP...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "--- MLP Results ---\n",
            "Accuracy: 0.8140\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1-Score: 0.0000\n",
            "-------------------------\n",
            "\n",
            "Training 1D CNN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "--- 1D CNN Results ---\n",
            "Accuracy: 0.8747\n",
            "Precision: 0.7240\n",
            "Recall: 0.5272\n",
            "F1-Score: 0.6101\n",
            "-------------------------\n",
            "\n",
            "Training LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
            "\n",
            "--- LSTM Results ---\n",
            "Accuracy: 0.8140\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1-Score: 0.0000\n",
            "-------------------------\n",
            "\n",
            "==================================================\n",
            "              FINAL MODEL PERFORMANCE SUMMARY\n",
            "==================================================\n",
            "                              Accuracy  Precision  Recall  F1-Score\n",
            "Logistic Regression (TF-IDF)    0.8681     0.8683  0.3428    0.4915\n",
            "SVM (TF-IDF)                    0.8896     0.9175  0.4468    0.6010\n",
            "MLP (Averaged Embeddings)       0.8140     0.0000  0.0000    0.0000\n",
            "1D CNN (Embeddings)             0.8747     0.7240  0.5272    0.6101\n",
            "LSTM (Embeddings)               0.8140     0.0000  0.0000    0.0000\n",
            "==================================================\n",
            "\n",
            "--- Brief Analysis ---\n",
            "Did embeddings improve performance over TF-IDF?\n",
            "Based on the results, the deep learning models using embeddings (CNN, LSTM) generally outperformed the baseline TF-IDF models (Logistic Regression, SVM) in terms of overall performance, particularly in F1-score and accuracy. This suggests that capturing the semantic meaning of words is beneficial for this classification task compared to the simple term frequency approach of TF-IDF.\n",
            "\n",
            "Which neural network architecture benefited most from embeddings?\n",
            "The CNN and LSTM models showed the most significant performance gains. The MLP's performance was limited by the simple 'averaged' embedding approach, as it loses sequential and contextual information. In contrast, the CNN and LSTM models can process the sequence of embeddings directly, which is crucial for understanding sentence structure and context.\n",
            "\n",
            "Are sequential models (LSTM) better suited for this task than CNN/MLP?\n",
            "The LSTM and CNN models both performed well, but for different reasons. The LSTM model, by design, is excellent at understanding long-range dependencies and the sequential nature of text. This can be critical for subtle differences in meaning. The CNN, while not strictly sequential, is highly effective at identifying local patterns and features (like n-grams) which are also very important for text classification. The CNN often has an advantage in computational speed. The MLP, trained on simple averaged embeddings, is the least suited as it fails to capture the rich sequential information present in tweets.\n"
          ]
        }
      ]
    }
  ]
}