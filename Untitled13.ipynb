{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYLxZZyz7A91aQ1e65NwSu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harivamsh2005/NLP/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Sentiment Analysis with Word2Vec/GloVe + Deep Learning\n",
        "# Models: LSTM, CNN, Bi-LSTM\n",
        "# ================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional\n",
        "\n",
        "# --------------------\n",
        "# Step 1: Load Dataset\n",
        "# --------------------\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "df = pd.read_csv(\"tweets.csv\")\n",
        "print(\"Columns found in dataset:\", df.columns)\n",
        "\n",
        "# Detect text column\n",
        "if \"tweet\" in df.columns:\n",
        "    text_col = \"tweet\"\n",
        "elif \"text\" in df.columns:\n",
        "    text_col = \"text\"\n",
        "elif \"content\" in df.columns:\n",
        "    text_col = \"content\"\n",
        "else:\n",
        "    text_col = df.columns[0]   # assume first col is text\n",
        "\n",
        "# Detect label column\n",
        "if \"label\" in df.columns:\n",
        "    label_col = \"label\"\n",
        "elif \"sentiment\" in df.columns:\n",
        "    label_col = \"sentiment\"\n",
        "elif \"target\" in df.columns:\n",
        "    label_col = \"target\"\n",
        "elif \"class\" in df.columns:\n",
        "    label_col = \"class\"\n",
        "else:\n",
        "    label_col = df.columns[1]  # assume second col is label\n",
        "\n",
        "print(f\"Using text column: {text_col}, label column: {label_col}\")\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [w for w in text if w not in stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "df[\"clean_tweet\"] = df[text_col].apply(clean_text)\n",
        "\n",
        "X = df[\"clean_tweet\"].values\n",
        "y = df[label_col].values\n",
        "\n",
        "# Convert labels if they are strings (\"positive\"/\"negative\")\n",
        "if y.dtype == \"O\":\n",
        "    y = np.where(y.str.lower().isin([\"positive\", \"pos\", \"1\"]), 1, 0)\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Tokenization + Pad\n",
        "# -------------------------\n",
        "max_vocab = 20000\n",
        "max_len = 30\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(X_seq, maxlen=max_len)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 3: Load GloVe Embeddings\n",
        "# ------------------------------\n",
        "embedding_index = {}\n",
        "with open(\"glove.6B.300d.txt\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embedding_index[word] = vector\n",
        "\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((max_vocab, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < max_vocab:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# ------------------------\n",
        "# Step 4: Model Functions\n",
        "# ------------------------\n",
        "def build_lstm():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_vocab, embedding_dim, weights=[embedding_matrix],\n",
        "                        input_length=max_len, trainable=False))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_cnn():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_vocab, embedding_dim, weights=[embedding_matrix],\n",
        "                        input_length=max_len, trainable=False))\n",
        "    model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_bilstm():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_vocab, embedding_dim, weights=[embedding_matrix],\n",
        "                        input_length=max_len, trainable=False))\n",
        "    model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# -----------------------\n",
        "# Step 5: Train & Evaluate\n",
        "# -----------------------\n",
        "models = {\"LSTM\": build_lstm(), \"CNN\": build_cnn(), \"BiLSTM\": build_bilstm()}\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "              epochs=5, batch_size=64, verbose=1)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\"Accuracy\": acc, \"F1\": f1}\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# -----------------------\n",
        "# Step 6: Error Analysis\n",
        "# -----------------------\n",
        "def error_analysis(model, X_test, y_test, name):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
        "    errors = []\n",
        "    for i in range(len(y_test)):\n",
        "        if y_pred[i] != y_test[i]:\n",
        "            errors.append((df.iloc[i][text_col], y_test[i], y_pred[i]))\n",
        "    print(f\"\\nMisclassified Positive tweets by {name}:\")\n",
        "    for t, true, pred in errors[:5]:\n",
        "        if true == 1 and pred == 0:\n",
        "            print(\"Tweet:\", t)\n",
        "    print(f\"\\nMisclassified Negative tweets by {name}:\")\n",
        "    for t, true, pred in errors[:5]:\n",
        "        if true == 0 and pred == 1:\n",
        "            print(\"Tweet:\", t)\n",
        "\n",
        "error_analysis(models[\"LSTM\"], X_test, y_test, \"LSTM\")\n",
        "\n",
        "# -----------------------\n",
        "# Step 7: Compare with ML\n",
        "# -----------------------\n",
        "print(\"\\n=== Deep Learning Results ===\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}: Accuracy={metrics['Accuracy']:.4f}, F1={metrics['F1']:.4f}\")\n",
        "\n",
        "# Example from old assignment\n",
        "traditional_results = {\"SVM\": {\"Accuracy\": 0.78, \"F1\": 0.76},\n",
        "                       \"NaiveBayes\": {\"Accuracy\": 0.74, \"F1\": 0.72}}\n",
        "print(\"\\n=== Traditional ML Results ===\")\n",
        "print(traditional_results)\n",
        "\n",
        "# -----------------------\n",
        "# Step 8: Conclusion\n",
        "# -----------------------\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"Deep learning models (especially Bi-LSTM) generally outperform traditional ML models on sentiment detection when using pre-trained embeddings.\")\n",
        "print(\"CNN is faster and competitive, while traditional ML is useful only for very small datasets.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0FqD8CPVzXH",
        "outputId": "308233de-0822-4e2a-e914-c6d5b89f9416"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns found in dataset: Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
            "Using text column: text, label column: target\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training LSTM...\n",
            "Epoch 1/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 166ms/step - accuracy: 0.8419 - loss: 0.3996 - val_accuracy: 0.8729 - val_loss: 0.3211\n",
            "Epoch 2/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 159ms/step - accuracy: 0.8777 - loss: 0.3047 - val_accuracy: 0.8914 - val_loss: 0.2838\n",
            "Epoch 3/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 156ms/step - accuracy: 0.8812 - loss: 0.2751 - val_accuracy: 0.8993 - val_loss: 0.2665\n",
            "Epoch 4/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 153ms/step - accuracy: 0.8990 - loss: 0.2498 - val_accuracy: 0.9006 - val_loss: 0.2659\n",
            "Epoch 5/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 147ms/step - accuracy: 0.9046 - loss: 0.2314 - val_accuracy: 0.8997 - val_loss: 0.2624\n",
            "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step\n",
            "\n",
            "LSTM Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94      1878\n",
            "           1       0.78      0.59      0.67       396\n",
            "\n",
            "    accuracy                           0.90      2274\n",
            "   macro avg       0.85      0.78      0.81      2274\n",
            "weighted avg       0.89      0.90      0.89      2274\n",
            "\n",
            "\n",
            "Training CNN...\n",
            "Epoch 1/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - accuracy: 0.8469 - loss: 0.3632 - val_accuracy: 0.9006 - val_loss: 0.2649\n",
            "Epoch 2/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 54ms/step - accuracy: 0.9423 - loss: 0.1767 - val_accuracy: 0.8997 - val_loss: 0.2585\n",
            "Epoch 3/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.9768 - loss: 0.1016 - val_accuracy: 0.9120 - val_loss: 0.2518\n",
            "Epoch 4/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.9924 - loss: 0.0533 - val_accuracy: 0.9077 - val_loss: 0.2614\n",
            "Epoch 5/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - accuracy: 0.9954 - loss: 0.0358 - val_accuracy: 0.9112 - val_loss: 0.2882\n",
            "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "\n",
            "CNN Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95      1878\n",
            "           1       0.84      0.60      0.70       396\n",
            "\n",
            "    accuracy                           0.91      2274\n",
            "   macro avg       0.88      0.79      0.83      2274\n",
            "weighted avg       0.91      0.91      0.91      2274\n",
            "\n",
            "\n",
            "Training BiLSTM...\n",
            "Epoch 1/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 332ms/step - accuracy: 0.8274 - loss: 0.4026 - val_accuracy: 0.8945 - val_loss: 0.2775\n",
            "Epoch 2/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 316ms/step - accuracy: 0.8786 - loss: 0.2972 - val_accuracy: 0.8835 - val_loss: 0.2912\n",
            "Epoch 3/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 344ms/step - accuracy: 0.8934 - loss: 0.2665 - val_accuracy: 0.8989 - val_loss: 0.2657\n",
            "Epoch 4/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 337ms/step - accuracy: 0.9018 - loss: 0.2344 - val_accuracy: 0.8971 - val_loss: 0.2649\n",
            "Epoch 5/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 334ms/step - accuracy: 0.9129 - loss: 0.2113 - val_accuracy: 0.8927 - val_loss: 0.2750\n",
            "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step\n",
            "\n",
            "BiLSTM Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.94      0.94      1878\n",
            "           1       0.71      0.66      0.68       396\n",
            "\n",
            "    accuracy                           0.89      2274\n",
            "   macro avg       0.82      0.80      0.81      2274\n",
            "weighted avg       0.89      0.89      0.89      2274\n",
            "\n",
            "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\n",
            "Misclassified Positive tweets by LSTM:\n",
            "Tweet: Several houses have been set ablaze in Ngemsibaa village, Oku sub division in the North West Region of Cameroon byâ€¦ https://t.co/99uHGAzxy2\n",
            "Tweet: Images showing the havoc caused by the #Cameroon military as they torched houses in #Oku.The shameless military is reportedâ€¦\n",
            "Tweet: Rengoku sets my heart ablazeğŸ˜”â¤ï¸ğŸ”¥ P.s. I missed this style of coloring I do so here it is c: #é¬¼æ»…ã®åˆƒ https://t.co/YrUF9g68s0\n",
            "Tweet: ğŸ“· Heartfelt appreciation to Prime Minister YAB Tun Dr. wife, YABhg. Tun Dr. Siti Hasmah Mohd Ali foâ€¦ https://t.co/YOwUp1BYUP\n",
            "\n",
            "Misclassified Negative tweets by LSTM:\n",
            "Tweet: Marivan, Kurdistan Province Monday, Jan 13th, 2020 Protesters set the propaganda banner of #QassemSoleimani ablazeâ€¦ https://t.co/IVGCYJZlmK\n",
            "\n",
            "=== Deep Learning Results ===\n",
            "LSTM: Accuracy=0.8997, F1=0.6734\n",
            "CNN: Accuracy=0.9112, F1=0.7029\n",
            "BiLSTM: Accuracy=0.8927, F1=0.6806\n",
            "\n",
            "=== Traditional ML Results ===\n",
            "{'SVM': {'Accuracy': 0.78, 'F1': 0.76}, 'NaiveBayes': {'Accuracy': 0.74, 'F1': 0.72}}\n",
            "\n",
            "Conclusion:\n",
            "Deep learning models (especially Bi-LSTM) generally outperform traditional ML models on sentiment detection when using pre-trained embeddings.\n",
            "CNN is faster and competitive, while traditional ML is useful only for very small datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, zipfile, requests\n",
        "\n",
        "glove_path = \"glove.6B.300d.txt\"\n",
        "\n",
        "if not os.path.exists(glove_path):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    r = requests.get(url)\n",
        "    open(\"glove.6B.zip\", \"wb\").write(r.content)\n",
        "\n",
        "    with zipfile.ZipFile(\"glove.6B.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "\n",
        "    print(\"GloVe downloaded and extracted!\")\n",
        "\n",
        "else:\n",
        "    print(\"GloVe file already exists.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYUF2hIwZLsx",
        "outputId": "a4b0da56-feaa-4030-afe5-e071ca4bc291"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings...\n",
            "GloVe downloaded and extracted!\n"
          ]
        }
      ]
    }
  ]
}