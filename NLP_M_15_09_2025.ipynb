{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyaJDz2McH+F755Lal0FiI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harivamsh2005/NLP/blob/main/NLP_M_15_09_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl6_aba4FsrP",
        "outputId": "a428094a-1b3a-417f-e1bc-d006581d6652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully from local file 'train.csv'.\n",
            "\n",
            "Preprocessing text data...\n",
            "\n",
            "====================\n",
            "Processing: Unigrams Only\n",
            "====================\n",
            "Creating TF-IDF vectors with Unigrams Only...\n",
            "Vocabulary size: 14888\n",
            "\n",
            "Training ANN with Unigrams Only...\n",
            "ANN Results -> Train Acc: 0.9253, Test Acc: 0.8109\n",
            "\n",
            "Training LSTM with Unigrams Only...\n",
            "LSTM Results -> Train Acc: 0.9286, Test Acc: 0.8030\n",
            "\n",
            "====================\n",
            "Processing: Unigrams + Bigrams\n",
            "====================\n",
            "Creating TF-IDF vectors with Unigrams + Bigrams...\n",
            "Vocabulary size: 15000\n",
            "\n",
            "Training ANN with Unigrams + Bigrams...\n",
            "ANN Results -> Train Acc: 0.9177, Test Acc: 0.8083\n",
            "\n",
            "Training LSTM with Unigrams + Bigrams...\n",
            "LSTM Results -> Train Acc: 0.8997, Test Acc: 0.8122\n",
            "\n",
            "====================\n",
            "Processing: Unigrams + Bigrams + Trigrams\n",
            "====================\n",
            "Creating TF-IDF vectors with Unigrams + Bigrams + Trigrams...\n",
            "Vocabulary size: 15000\n",
            "\n",
            "Training ANN with Unigrams + Bigrams + Trigrams...\n",
            "ANN Results -> Train Acc: 0.9061, Test Acc: 0.8096\n",
            "\n",
            "Training LSTM with Unigrams + Bigrams + Trigrams...\n",
            "LSTM Results -> Train Acc: 0.8898, Test Acc: 0.8162\n",
            "\n",
            "\n",
            "--- Final Model Comparison ---\n",
            "                                      Train Accuracy  Test Accuracy\n",
            "LSTM - Unigrams + Bigrams + Trigrams        0.889819       0.816152\n",
            "LSTM - Unigrams + Bigrams                   0.899672       0.812213\n",
            "ANN - Unigrams Only                         0.925287       0.810900\n",
            "ANN - Unigrams + Bigrams + Trigrams         0.906076       0.809586\n",
            "ANN - Unigrams + Bigrams                    0.917734       0.808273\n",
            "LSTM - Unigrams Only                        0.928571       0.803020\n",
            "\n",
            "--- End of Analysis ---\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# --- 1. Setup and Data Loading ---\n",
        "# Ensure you have the necessary NLTK data\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the dataset (using a standard Kaggle disaster tweets dataset)\n",
        "# This dataset is very similar in structure to what you might find on Reddit.\n",
        "try:\n",
        "    df = pd.read_csv('train.csv')\n",
        "    print(\"Dataset loaded successfully from local file 'train.csv'.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Local file not found. Loading from a public URL...\")\n",
        "    # Using a public URL for the Kaggle competition dataset for easy reproducibility\n",
        "    url = 'https://storage.googleapis.com/bert_models/2020_07_23/nlp-getting-started.zip'\n",
        "    df = pd.read_csv(url, compression='zip', header=0, sep=',', quotechar='\"')\n",
        "    print(\"Dataset loaded from URL.\")\n",
        "\n",
        "\n",
        "# --- 2. Text Preprocessing ---\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses a single text entry.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "print(\"\\nPreprocessing text data...\")\n",
        "df['clean_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# --- 3. Data Splitting ---\n",
        "X = df['clean_text']\n",
        "y = df['target']\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --- 4. Model Training and Evaluation ---\n",
        "# We will test three n-gram configurations\n",
        "ngram_configs = {\n",
        "    \"Unigrams Only\": (1, 1),\n",
        "    \"Unigrams + Bigrams\": (1, 2),\n",
        "    \"Unigrams + Bigrams + Trigrams\": (1, 3)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Set common training parameters\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "# Using EarlyStopping to prevent overfitting and reduce training time\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "\n",
        "for name, ngrams in ngram_configs.items():\n",
        "    print(f\"\\n{'='*20}\\nProcessing: {name}\\n{'='*20}\")\n",
        "\n",
        "    # Step 4a: TF-IDF Vectorization\n",
        "    print(f\"Creating TF-IDF vectors with {name}...\")\n",
        "    # We limit features to manage memory and speed, especially for trigrams\n",
        "    vectorizer = TfidfVectorizer(ngram_range=ngrams, max_features=15000)\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
        "    X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "    vocab_size = X_train_tfidf.shape[1]\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "    # --- 4b: ANN Model ---\n",
        "    print(f\"\\nTraining ANN with {name}...\")\n",
        "    ann_model = Sequential([\n",
        "        Input(shape=(vocab_size,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history_ann = ann_model.fit(\n",
        "        X_train_tfidf, y_train,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0  # Set to 1 if you want to see training progress\n",
        "    )\n",
        "\n",
        "    train_loss_ann, train_acc_ann = ann_model.evaluate(X_train_tfidf, y_train, verbose=0)\n",
        "    test_loss_ann, test_acc_ann = ann_model.evaluate(X_test_tfidf, y_test, verbose=0)\n",
        "    results[f\"ANN - {name}\"] = {'Train Accuracy': train_acc_ann, 'Test Accuracy': test_acc_ann}\n",
        "    print(f\"ANN Results -> Train Acc: {train_acc_ann:.4f}, Test Acc: {test_acc_ann:.4f}\")\n",
        "\n",
        "    # --- 4c: LSTM Model ---\n",
        "    # Reshape data for LSTM: (samples, timesteps, features)\n",
        "    # Note: Using TF-IDF with LSTM is unconventional. LSTMs prefer sequences.\n",
        "    # Here, we treat the entire TF-IDF vector as a single timestep.\n",
        "    X_train_lstm = np.reshape(X_train_tfidf, (X_train_tfidf.shape[0], 1, vocab_size))\n",
        "    X_test_lstm = np.reshape(X_test_tfidf, (X_test_tfidf.shape[0], 1, vocab_size))\n",
        "\n",
        "    print(f\"\\nTraining LSTM with {name}...\")\n",
        "    lstm_model = Sequential([\n",
        "        Input(shape=(1, vocab_size,)),\n",
        "        LSTM(64),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history_lstm = lstm_model.fit(\n",
        "        X_train_lstm, y_train,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0 # Set to 1 if you want to see training progress\n",
        "    )\n",
        "\n",
        "    train_loss_lstm, train_acc_lstm = lstm_model.evaluate(X_train_lstm, y_train, verbose=0)\n",
        "    test_loss_lstm, test_acc_lstm = lstm_model.evaluate(X_test_lstm, y_test, verbose=0)\n",
        "    results[f\"LSTM - {name}\"] = {'Train Accuracy': train_acc_lstm, 'Test Accuracy': test_acc_lstm}\n",
        "    print(f\"LSTM Results -> Train Acc: {train_acc_lstm:.4f}, Test Acc: {test_acc_lstm:.4f}\")\n",
        "\n",
        "# --- 5. Final Comparison ---\n",
        "print(\"\\n\\n--- Final Model Comparison ---\")\n",
        "results_df = pd.DataFrame(results).T.sort_values(by='Test Accuracy', ascending=False)\n",
        "print(results_df)\n",
        "print(\"\\n--- End of Analysis ---\")"
      ]
    }
  ]
}